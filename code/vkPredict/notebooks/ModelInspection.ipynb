{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../'))\n",
    "\n",
    "import peewee as pw\n",
    "from toyDb.databases import ExperimentDb, ShaderDb\n",
    "from toyDb.utils.Directory import getToyDbRootDir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "ExperimentDb.init_from_default_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.HfTracedSpvTokenizer import HfTracedSpvTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HfTracedSpvTokenizer(single_entrypoint=False)\n",
    "\n",
    "config = transformers.RobertaConfig(**{\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"max_position_embeddings\": 4096,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"type_vocab_size\": 1,\n",
    "    \"vocab_size\": 40000\n",
    "})\n",
    "\n",
    "config.num_labels = 1\n",
    "config.problem_type = \"regression\"\n",
    "\n",
    "model = transformers.RobertaForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment selected: libreliu-GCL-Arch -  Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz - NVIDIA GeForce RTX 3060 - NVIDIA 535.113.01\n"
     ]
    }
   ],
   "source": [
    "selectedEnv = ExperimentDb.Environment.select()[0]\n",
    "print(f\"Environment selected: {selectedEnv.node} - {selectedEnv.cpu} - {selectedEnv.gpu} - {selectedEnv.gpu_driver}\")\n",
    "\n",
    "# https://shadertoy.com/view/lllBR7\n",
    "# https://shadertoy.com/view/3ttBWN\n",
    "candidateShaders = ['lllBR7', '3ttBWN']\n",
    "query = ExperimentDb.ImageOnlyExperiment.select(\n",
    "      ExperimentDb.ImageOnlyExperiment\n",
    "    ).where(\n",
    "      # Canonical condition\n",
    "      ExperimentDb.ImageOnlyExperiment.num_cycles == ExperimentDb.CANONICAL_NUM_CYCLES,\n",
    "      ExperimentDb.ImageOnlyExperiment.num_trials == ExperimentDb.CANONICAL_NUM_TRIALS,\n",
    "      ExperimentDb.ImageOnlyExperiment.width == ExperimentDb.CANONICAL_WIDTH,\n",
    "      ExperimentDb.ImageOnlyExperiment.height == ExperimentDb.CANONICAL_HEIGHT,\n",
    "      # Inside all_measurable_and_traceable_canonical_shaders\n",
    "      ExperimentDb.ImageOnlyExperiment.environment_id == selectedEnv,\n",
    "      ExperimentDb.ImageOnlyExperiment.shader_shadertoy_id.in_(candidateShaders)\n",
    "    ).order_by(\n",
    "      ExperimentDb.ImageOnlyExperiment.shader_shadertoy_id\n",
    "    )\n",
    "\n",
    "assert(len(query) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expr[0] = {'input_ids': 1132, 'trace_labels': 1132, 'attention_mask': 1132, 'position_ids': 1132}\n",
      "expr[1] = {'input_ids': 1038, 'trace_labels': 1038, 'attention_mask': 1038, 'position_ids': 1038}\n",
      "{'input_ids': tensor([[ 1001,  2019, 20002,  ..., 20012, 20194,  2253],\n",
      "        [ 1000,  1000,  1000,  ..., 20024, 20207,  2253]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'position_ids': tensor([[   0,    1,    2,  ..., 1129, 1130, 1131],\n",
      "        [   0,    0,    0,  ..., 1035, 1036, 1037]])}\n",
      "{'input_ids': torch.Size([2, 1132]), 'attention_mask': torch.Size([2, 1132]), 'position_ids': torch.Size([2, 1132])}\n"
     ]
    }
   ],
   "source": [
    "dataCollator = transformers.data.DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
    "\n",
    "batch = []\n",
    "\n",
    "for exprIdx, expr in enumerate(query):\n",
    "    results = json.loads(expr.results)\n",
    "    result_mean = sum(results) / len(results)\n",
    "\n",
    "    data = {\n",
    "        \"shaderId\": expr.shader.shader_id,\n",
    "        # SPIR-V bytes\n",
    "        \"fragSpv\": expr.shader.fragment_spv,\n",
    "        # SPIR-V bytes\n",
    "        \"traceFragSpv\": expr.trace.traced_fragment_spv,\n",
    "        # float\n",
    "        \"timeMean\": result_mean,\n",
    "        # dict[int, int]\n",
    "        \"bbIdxMap\": {int(k): v for k, v in json.loads(expr.trace.bb_idx_map).items()},\n",
    "        # List[int]\n",
    "        \"bbTraceCounters\": json.loads(expr.trace.bb_trace_counters)\n",
    "    }\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        spvBinaryRepr=data[\"fragSpv\"],\n",
    "        id2TraceIdxMap=data[\"bbIdxMap\"],\n",
    "        traceCounters=data[\"bbTraceCounters\"]\n",
    "    )\n",
    "    encoded_inputs_stats = {k: len(v) for k, v in encoded_inputs.items()}\n",
    "\n",
    "    print(f\"expr[{exprIdx}] = {encoded_inputs_stats}\")\n",
    "\n",
    "    del encoded_inputs['trace_labels']\n",
    "    batch.append(encoded_inputs)\n",
    "\n",
    "collated = dataCollator(batch)\n",
    "print(collated)\n",
    "\n",
    "collated_stats = {k: v.shape for k, v in collated.items()}\n",
    "print(collated_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => Use Python debugger here to inspect the model\n",
    "\n",
    "# model.eval()\n",
    "# model(**collated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfformer model test:\n",
    "\n",
    "Configure Perfformer model as roberta-base model and check if we can do MLM / other predictions correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31414, 232, 2]\n",
      "[0, 20920, 232, 2]\n"
     ]
    }
   ],
   "source": [
    "from model.modeling_perfformer import PerfformerForMaskedLM\n",
    "from model.configuration_perfformer import PerfformerConfig\n",
    "\n",
    "# point to somewhere you cloned\n",
    "robertaBaseDir = r\"C:\\Projects\\roberta-playground\\roberta-base\"\n",
    "robertaTokenizer = transformers.RobertaTokenizer.from_pretrained(robertaBaseDir)\n",
    "\n",
    "print(robertaTokenizer(\"Hello world\")[\"input_ids\"])\n",
    "print(robertaTokenizer(\" Hello world\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load as roberta-base\n",
    "perfformerCfgDict = {\n",
    "    \"architectures\": [\n",
    "      \"RobertaForMaskedLM\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"bos_token_id\": 0,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"max_position_embeddings\": 514,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"pad_token_id\": 1,\n",
    "    \"type_vocab_size\": 1,\n",
    "    \"vocab_size\": 50265,\n",
    "    # == The following option differs! ==\n",
    "    \"attention_type\": \"vanilla\",\n",
    "    # BERT-like\n",
    "    \"position_embedding_type\": \"absolute-learnable\"\n",
    "}\n",
    "\n",
    "robertaCfgDict = {\n",
    "    \"architectures\": [\n",
    "      \"RobertaForMaskedLM\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"bos_token_id\": 0,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"max_position_embeddings\": 514,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"pad_token_id\": 1,\n",
    "    \"type_vocab_size\": 1,\n",
    "    \"vocab_size\": 50265,\n",
    "    # BERT-like\n",
    "    \"position_embedding_type\": \"absolute\"\n",
    "}\n",
    "\n",
    "perfformerCfg = PerfformerConfig(**perfformerCfgDict)\n",
    "robertaCfg = transformers.RobertaConfig(**robertaCfgDict)\n",
    "\n",
    "perfformerModel = PerfformerForMaskedLM(perfformerCfg)\n",
    "robertaModel = transformers.RobertaForMaskedLM(robertaCfg)\n",
    "# perfformerModel = PerfformerForMaskedLM.from_pretrained(robertaBaseDir, config=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the model structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PerfformerForMaskedLM(\n",
      "  (perfformer): PerfformerModel(\n",
      "    (embeddings): PerfformerEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): PerfformerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x PerfformerLayer(\n",
      "          (attention): PerfformerAttention(\n",
      "            (self): PerfformerSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): PerfformerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): PerfformerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): PerfformerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): PerfformerLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(perfformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForMaskedLM(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(robertaModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example evaluation code for MLM:\n",
    "\n",
    "Method 1:\n",
    "\n",
    "```python\n",
    "robertaModelFromPretrained = transformers.RobertaForMaskedLM.from_pretrained(robertaBaseDir)\n",
    "robertaTokenizer = transformers.RobertaTokenizer.from_pretrained(robertaBaseDir)\n",
    "robertaPretrainedUnmasker = transformers.pipeline('fill-mask', model=robertaModelFromPretrained, tokenizer=robertaTokenizer)\n",
    "robertaPretrainedUnmasker(\"RoBERTa is a model developed by <mask>.\")\n",
    "```\n",
    "\n",
    "Method 2:\n",
    "\n",
    "```python\n",
    "# Make sure to put your model in evaluation mode\n",
    "perfformerModel.eval()\n",
    "\n",
    "# Let's test a masked sentence\n",
    "masked_sentence = \"RoBERTa is a model developed by <mask>.\"\n",
    "\n",
    "#tokenize the input\n",
    "input = robertaTokenizer.encode_plus(masked_sentence, return_tensors='pt')\n",
    "\n",
    "# get output from the model\n",
    "output = perfformerModel(**input)\n",
    "\n",
    "# get prediction\n",
    "predicted_index = torch.argmax(output.logits[0, input['input_ids'][0].tolist().index(robertaTokenizer.mask_token_id)]).item()\n",
    "\n",
    "# Decode prediction\n",
    "prediction = robertaTokenizer.decode([predicted_index])\n",
    "\n",
    "print(f\"The masked word is predicted as: {prediction}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine state dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load weight into robertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight\n",
      "roberta.embeddings.position_embeddings.weight\n",
      "roberta.embeddings.token_type_embeddings.weight\n",
      "roberta.embeddings.LayerNorm.weight\n",
      "roberta.embeddings.LayerNorm.bias\n",
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "roberta.pooler.dense.weight\n",
      "roberta.pooler.dense.bias\n",
      "lm_head.bias\n",
      "lm_head.dense.weight\n",
      "lm_head.dense.bias\n",
      "lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight\n",
      "\n",
      "_IncompatibleKeys(missing_keys=['roberta.embeddings.position_ids', 'lm_head.decoder.bias'], unexpected_keys=['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias'])\n"
     ]
    }
   ],
   "source": [
    "roberta_state_dict = torch.load(os.path.join(robertaBaseDir, \"./pytorch_model.bin\"), map_location=\"cpu\")\n",
    "roberta_state_dict_keys = roberta_state_dict.keys()\n",
    "\n",
    "for key in roberta_state_dict_keys:\n",
    "    print(key)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Will report missing and unexpected keys\n",
    "print(robertaModel.load_state_dict(roberta_state_dict, strict=False))\n",
    "robertaModel.tie_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: check if the layer weight names can be matched:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfformer.embeddings.word_embeddings.weight -> roberta.embeddings.word_embeddings.weight\n",
      "perfformer.embeddings.position_embeddings.weight -> roberta.embeddings.position_embeddings.weight\n",
      "perfformer.embeddings.token_type_embeddings.weight -> roberta.embeddings.token_type_embeddings.weight\n",
      "perfformer.embeddings.LayerNorm.weight -> roberta.embeddings.LayerNorm.weight\n",
      "perfformer.embeddings.LayerNorm.bias -> roberta.embeddings.LayerNorm.bias\n",
      "perfformer.encoder.layer.0.attention.self.query.weight -> roberta.encoder.layer.0.attention.self.query.weight\n",
      "perfformer.encoder.layer.0.attention.self.query.bias -> roberta.encoder.layer.0.attention.self.query.bias\n",
      "perfformer.encoder.layer.0.attention.self.key.weight -> roberta.encoder.layer.0.attention.self.key.weight\n",
      "perfformer.encoder.layer.0.attention.self.key.bias -> roberta.encoder.layer.0.attention.self.key.bias\n",
      "perfformer.encoder.layer.0.attention.self.value.weight -> roberta.encoder.layer.0.attention.self.value.weight\n",
      "perfformer.encoder.layer.0.attention.self.value.bias -> roberta.encoder.layer.0.attention.self.value.bias\n",
      "perfformer.encoder.layer.0.attention.output.dense.weight -> roberta.encoder.layer.0.attention.output.dense.weight\n",
      "perfformer.encoder.layer.0.attention.output.dense.bias -> roberta.encoder.layer.0.attention.output.dense.bias\n",
      "perfformer.encoder.layer.0.attention.output.LayerNorm.weight -> roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.0.attention.output.LayerNorm.bias -> roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.0.intermediate.dense.weight -> roberta.encoder.layer.0.intermediate.dense.weight\n",
      "perfformer.encoder.layer.0.intermediate.dense.bias -> roberta.encoder.layer.0.intermediate.dense.bias\n",
      "perfformer.encoder.layer.0.output.dense.weight -> roberta.encoder.layer.0.output.dense.weight\n",
      "perfformer.encoder.layer.0.output.dense.bias -> roberta.encoder.layer.0.output.dense.bias\n",
      "perfformer.encoder.layer.0.output.LayerNorm.weight -> roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.0.output.LayerNorm.bias -> roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.1.attention.self.query.weight -> roberta.encoder.layer.1.attention.self.query.weight\n",
      "perfformer.encoder.layer.1.attention.self.query.bias -> roberta.encoder.layer.1.attention.self.query.bias\n",
      "perfformer.encoder.layer.1.attention.self.key.weight -> roberta.encoder.layer.1.attention.self.key.weight\n",
      "perfformer.encoder.layer.1.attention.self.key.bias -> roberta.encoder.layer.1.attention.self.key.bias\n",
      "perfformer.encoder.layer.1.attention.self.value.weight -> roberta.encoder.layer.1.attention.self.value.weight\n",
      "perfformer.encoder.layer.1.attention.self.value.bias -> roberta.encoder.layer.1.attention.self.value.bias\n",
      "perfformer.encoder.layer.1.attention.output.dense.weight -> roberta.encoder.layer.1.attention.output.dense.weight\n",
      "perfformer.encoder.layer.1.attention.output.dense.bias -> roberta.encoder.layer.1.attention.output.dense.bias\n",
      "perfformer.encoder.layer.1.attention.output.LayerNorm.weight -> roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.1.attention.output.LayerNorm.bias -> roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.1.intermediate.dense.weight -> roberta.encoder.layer.1.intermediate.dense.weight\n",
      "perfformer.encoder.layer.1.intermediate.dense.bias -> roberta.encoder.layer.1.intermediate.dense.bias\n",
      "perfformer.encoder.layer.1.output.dense.weight -> roberta.encoder.layer.1.output.dense.weight\n",
      "perfformer.encoder.layer.1.output.dense.bias -> roberta.encoder.layer.1.output.dense.bias\n",
      "perfformer.encoder.layer.1.output.LayerNorm.weight -> roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.1.output.LayerNorm.bias -> roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.2.attention.self.query.weight -> roberta.encoder.layer.2.attention.self.query.weight\n",
      "perfformer.encoder.layer.2.attention.self.query.bias -> roberta.encoder.layer.2.attention.self.query.bias\n",
      "perfformer.encoder.layer.2.attention.self.key.weight -> roberta.encoder.layer.2.attention.self.key.weight\n",
      "perfformer.encoder.layer.2.attention.self.key.bias -> roberta.encoder.layer.2.attention.self.key.bias\n",
      "perfformer.encoder.layer.2.attention.self.value.weight -> roberta.encoder.layer.2.attention.self.value.weight\n",
      "perfformer.encoder.layer.2.attention.self.value.bias -> roberta.encoder.layer.2.attention.self.value.bias\n",
      "perfformer.encoder.layer.2.attention.output.dense.weight -> roberta.encoder.layer.2.attention.output.dense.weight\n",
      "perfformer.encoder.layer.2.attention.output.dense.bias -> roberta.encoder.layer.2.attention.output.dense.bias\n",
      "perfformer.encoder.layer.2.attention.output.LayerNorm.weight -> roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.2.attention.output.LayerNorm.bias -> roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.2.intermediate.dense.weight -> roberta.encoder.layer.2.intermediate.dense.weight\n",
      "perfformer.encoder.layer.2.intermediate.dense.bias -> roberta.encoder.layer.2.intermediate.dense.bias\n",
      "perfformer.encoder.layer.2.output.dense.weight -> roberta.encoder.layer.2.output.dense.weight\n",
      "perfformer.encoder.layer.2.output.dense.bias -> roberta.encoder.layer.2.output.dense.bias\n",
      "perfformer.encoder.layer.2.output.LayerNorm.weight -> roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.2.output.LayerNorm.bias -> roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.3.attention.self.query.weight -> roberta.encoder.layer.3.attention.self.query.weight\n",
      "perfformer.encoder.layer.3.attention.self.query.bias -> roberta.encoder.layer.3.attention.self.query.bias\n",
      "perfformer.encoder.layer.3.attention.self.key.weight -> roberta.encoder.layer.3.attention.self.key.weight\n",
      "perfformer.encoder.layer.3.attention.self.key.bias -> roberta.encoder.layer.3.attention.self.key.bias\n",
      "perfformer.encoder.layer.3.attention.self.value.weight -> roberta.encoder.layer.3.attention.self.value.weight\n",
      "perfformer.encoder.layer.3.attention.self.value.bias -> roberta.encoder.layer.3.attention.self.value.bias\n",
      "perfformer.encoder.layer.3.attention.output.dense.weight -> roberta.encoder.layer.3.attention.output.dense.weight\n",
      "perfformer.encoder.layer.3.attention.output.dense.bias -> roberta.encoder.layer.3.attention.output.dense.bias\n",
      "perfformer.encoder.layer.3.attention.output.LayerNorm.weight -> roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.3.attention.output.LayerNorm.bias -> roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.3.intermediate.dense.weight -> roberta.encoder.layer.3.intermediate.dense.weight\n",
      "perfformer.encoder.layer.3.intermediate.dense.bias -> roberta.encoder.layer.3.intermediate.dense.bias\n",
      "perfformer.encoder.layer.3.output.dense.weight -> roberta.encoder.layer.3.output.dense.weight\n",
      "perfformer.encoder.layer.3.output.dense.bias -> roberta.encoder.layer.3.output.dense.bias\n",
      "perfformer.encoder.layer.3.output.LayerNorm.weight -> roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.3.output.LayerNorm.bias -> roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.4.attention.self.query.weight -> roberta.encoder.layer.4.attention.self.query.weight\n",
      "perfformer.encoder.layer.4.attention.self.query.bias -> roberta.encoder.layer.4.attention.self.query.bias\n",
      "perfformer.encoder.layer.4.attention.self.key.weight -> roberta.encoder.layer.4.attention.self.key.weight\n",
      "perfformer.encoder.layer.4.attention.self.key.bias -> roberta.encoder.layer.4.attention.self.key.bias\n",
      "perfformer.encoder.layer.4.attention.self.value.weight -> roberta.encoder.layer.4.attention.self.value.weight\n",
      "perfformer.encoder.layer.4.attention.self.value.bias -> roberta.encoder.layer.4.attention.self.value.bias\n",
      "perfformer.encoder.layer.4.attention.output.dense.weight -> roberta.encoder.layer.4.attention.output.dense.weight\n",
      "perfformer.encoder.layer.4.attention.output.dense.bias -> roberta.encoder.layer.4.attention.output.dense.bias\n",
      "perfformer.encoder.layer.4.attention.output.LayerNorm.weight -> roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.4.attention.output.LayerNorm.bias -> roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.4.intermediate.dense.weight -> roberta.encoder.layer.4.intermediate.dense.weight\n",
      "perfformer.encoder.layer.4.intermediate.dense.bias -> roberta.encoder.layer.4.intermediate.dense.bias\n",
      "perfformer.encoder.layer.4.output.dense.weight -> roberta.encoder.layer.4.output.dense.weight\n",
      "perfformer.encoder.layer.4.output.dense.bias -> roberta.encoder.layer.4.output.dense.bias\n",
      "perfformer.encoder.layer.4.output.LayerNorm.weight -> roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.4.output.LayerNorm.bias -> roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.5.attention.self.query.weight -> roberta.encoder.layer.5.attention.self.query.weight\n",
      "perfformer.encoder.layer.5.attention.self.query.bias -> roberta.encoder.layer.5.attention.self.query.bias\n",
      "perfformer.encoder.layer.5.attention.self.key.weight -> roberta.encoder.layer.5.attention.self.key.weight\n",
      "perfformer.encoder.layer.5.attention.self.key.bias -> roberta.encoder.layer.5.attention.self.key.bias\n",
      "perfformer.encoder.layer.5.attention.self.value.weight -> roberta.encoder.layer.5.attention.self.value.weight\n",
      "perfformer.encoder.layer.5.attention.self.value.bias -> roberta.encoder.layer.5.attention.self.value.bias\n",
      "perfformer.encoder.layer.5.attention.output.dense.weight -> roberta.encoder.layer.5.attention.output.dense.weight\n",
      "perfformer.encoder.layer.5.attention.output.dense.bias -> roberta.encoder.layer.5.attention.output.dense.bias\n",
      "perfformer.encoder.layer.5.attention.output.LayerNorm.weight -> roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.5.attention.output.LayerNorm.bias -> roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.5.intermediate.dense.weight -> roberta.encoder.layer.5.intermediate.dense.weight\n",
      "perfformer.encoder.layer.5.intermediate.dense.bias -> roberta.encoder.layer.5.intermediate.dense.bias\n",
      "perfformer.encoder.layer.5.output.dense.weight -> roberta.encoder.layer.5.output.dense.weight\n",
      "perfformer.encoder.layer.5.output.dense.bias -> roberta.encoder.layer.5.output.dense.bias\n",
      "perfformer.encoder.layer.5.output.LayerNorm.weight -> roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.5.output.LayerNorm.bias -> roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.6.attention.self.query.weight -> roberta.encoder.layer.6.attention.self.query.weight\n",
      "perfformer.encoder.layer.6.attention.self.query.bias -> roberta.encoder.layer.6.attention.self.query.bias\n",
      "perfformer.encoder.layer.6.attention.self.key.weight -> roberta.encoder.layer.6.attention.self.key.weight\n",
      "perfformer.encoder.layer.6.attention.self.key.bias -> roberta.encoder.layer.6.attention.self.key.bias\n",
      "perfformer.encoder.layer.6.attention.self.value.weight -> roberta.encoder.layer.6.attention.self.value.weight\n",
      "perfformer.encoder.layer.6.attention.self.value.bias -> roberta.encoder.layer.6.attention.self.value.bias\n",
      "perfformer.encoder.layer.6.attention.output.dense.weight -> roberta.encoder.layer.6.attention.output.dense.weight\n",
      "perfformer.encoder.layer.6.attention.output.dense.bias -> roberta.encoder.layer.6.attention.output.dense.bias\n",
      "perfformer.encoder.layer.6.attention.output.LayerNorm.weight -> roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.6.attention.output.LayerNorm.bias -> roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.6.intermediate.dense.weight -> roberta.encoder.layer.6.intermediate.dense.weight\n",
      "perfformer.encoder.layer.6.intermediate.dense.bias -> roberta.encoder.layer.6.intermediate.dense.bias\n",
      "perfformer.encoder.layer.6.output.dense.weight -> roberta.encoder.layer.6.output.dense.weight\n",
      "perfformer.encoder.layer.6.output.dense.bias -> roberta.encoder.layer.6.output.dense.bias\n",
      "perfformer.encoder.layer.6.output.LayerNorm.weight -> roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.6.output.LayerNorm.bias -> roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.7.attention.self.query.weight -> roberta.encoder.layer.7.attention.self.query.weight\n",
      "perfformer.encoder.layer.7.attention.self.query.bias -> roberta.encoder.layer.7.attention.self.query.bias\n",
      "perfformer.encoder.layer.7.attention.self.key.weight -> roberta.encoder.layer.7.attention.self.key.weight\n",
      "perfformer.encoder.layer.7.attention.self.key.bias -> roberta.encoder.layer.7.attention.self.key.bias\n",
      "perfformer.encoder.layer.7.attention.self.value.weight -> roberta.encoder.layer.7.attention.self.value.weight\n",
      "perfformer.encoder.layer.7.attention.self.value.bias -> roberta.encoder.layer.7.attention.self.value.bias\n",
      "perfformer.encoder.layer.7.attention.output.dense.weight -> roberta.encoder.layer.7.attention.output.dense.weight\n",
      "perfformer.encoder.layer.7.attention.output.dense.bias -> roberta.encoder.layer.7.attention.output.dense.bias\n",
      "perfformer.encoder.layer.7.attention.output.LayerNorm.weight -> roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.7.attention.output.LayerNorm.bias -> roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.7.intermediate.dense.weight -> roberta.encoder.layer.7.intermediate.dense.weight\n",
      "perfformer.encoder.layer.7.intermediate.dense.bias -> roberta.encoder.layer.7.intermediate.dense.bias\n",
      "perfformer.encoder.layer.7.output.dense.weight -> roberta.encoder.layer.7.output.dense.weight\n",
      "perfformer.encoder.layer.7.output.dense.bias -> roberta.encoder.layer.7.output.dense.bias\n",
      "perfformer.encoder.layer.7.output.LayerNorm.weight -> roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.7.output.LayerNorm.bias -> roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.8.attention.self.query.weight -> roberta.encoder.layer.8.attention.self.query.weight\n",
      "perfformer.encoder.layer.8.attention.self.query.bias -> roberta.encoder.layer.8.attention.self.query.bias\n",
      "perfformer.encoder.layer.8.attention.self.key.weight -> roberta.encoder.layer.8.attention.self.key.weight\n",
      "perfformer.encoder.layer.8.attention.self.key.bias -> roberta.encoder.layer.8.attention.self.key.bias\n",
      "perfformer.encoder.layer.8.attention.self.value.weight -> roberta.encoder.layer.8.attention.self.value.weight\n",
      "perfformer.encoder.layer.8.attention.self.value.bias -> roberta.encoder.layer.8.attention.self.value.bias\n",
      "perfformer.encoder.layer.8.attention.output.dense.weight -> roberta.encoder.layer.8.attention.output.dense.weight\n",
      "perfformer.encoder.layer.8.attention.output.dense.bias -> roberta.encoder.layer.8.attention.output.dense.bias\n",
      "perfformer.encoder.layer.8.attention.output.LayerNorm.weight -> roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.8.attention.output.LayerNorm.bias -> roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.8.intermediate.dense.weight -> roberta.encoder.layer.8.intermediate.dense.weight\n",
      "perfformer.encoder.layer.8.intermediate.dense.bias -> roberta.encoder.layer.8.intermediate.dense.bias\n",
      "perfformer.encoder.layer.8.output.dense.weight -> roberta.encoder.layer.8.output.dense.weight\n",
      "perfformer.encoder.layer.8.output.dense.bias -> roberta.encoder.layer.8.output.dense.bias\n",
      "perfformer.encoder.layer.8.output.LayerNorm.weight -> roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.8.output.LayerNorm.bias -> roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.9.attention.self.query.weight -> roberta.encoder.layer.9.attention.self.query.weight\n",
      "perfformer.encoder.layer.9.attention.self.query.bias -> roberta.encoder.layer.9.attention.self.query.bias\n",
      "perfformer.encoder.layer.9.attention.self.key.weight -> roberta.encoder.layer.9.attention.self.key.weight\n",
      "perfformer.encoder.layer.9.attention.self.key.bias -> roberta.encoder.layer.9.attention.self.key.bias\n",
      "perfformer.encoder.layer.9.attention.self.value.weight -> roberta.encoder.layer.9.attention.self.value.weight\n",
      "perfformer.encoder.layer.9.attention.self.value.bias -> roberta.encoder.layer.9.attention.self.value.bias\n",
      "perfformer.encoder.layer.9.attention.output.dense.weight -> roberta.encoder.layer.9.attention.output.dense.weight\n",
      "perfformer.encoder.layer.9.attention.output.dense.bias -> roberta.encoder.layer.9.attention.output.dense.bias\n",
      "perfformer.encoder.layer.9.attention.output.LayerNorm.weight -> roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.9.attention.output.LayerNorm.bias -> roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.9.intermediate.dense.weight -> roberta.encoder.layer.9.intermediate.dense.weight\n",
      "perfformer.encoder.layer.9.intermediate.dense.bias -> roberta.encoder.layer.9.intermediate.dense.bias\n",
      "perfformer.encoder.layer.9.output.dense.weight -> roberta.encoder.layer.9.output.dense.weight\n",
      "perfformer.encoder.layer.9.output.dense.bias -> roberta.encoder.layer.9.output.dense.bias\n",
      "perfformer.encoder.layer.9.output.LayerNorm.weight -> roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.9.output.LayerNorm.bias -> roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.10.attention.self.query.weight -> roberta.encoder.layer.10.attention.self.query.weight\n",
      "perfformer.encoder.layer.10.attention.self.query.bias -> roberta.encoder.layer.10.attention.self.query.bias\n",
      "perfformer.encoder.layer.10.attention.self.key.weight -> roberta.encoder.layer.10.attention.self.key.weight\n",
      "perfformer.encoder.layer.10.attention.self.key.bias -> roberta.encoder.layer.10.attention.self.key.bias\n",
      "perfformer.encoder.layer.10.attention.self.value.weight -> roberta.encoder.layer.10.attention.self.value.weight\n",
      "perfformer.encoder.layer.10.attention.self.value.bias -> roberta.encoder.layer.10.attention.self.value.bias\n",
      "perfformer.encoder.layer.10.attention.output.dense.weight -> roberta.encoder.layer.10.attention.output.dense.weight\n",
      "perfformer.encoder.layer.10.attention.output.dense.bias -> roberta.encoder.layer.10.attention.output.dense.bias\n",
      "perfformer.encoder.layer.10.attention.output.LayerNorm.weight -> roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.10.attention.output.LayerNorm.bias -> roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.10.intermediate.dense.weight -> roberta.encoder.layer.10.intermediate.dense.weight\n",
      "perfformer.encoder.layer.10.intermediate.dense.bias -> roberta.encoder.layer.10.intermediate.dense.bias\n",
      "perfformer.encoder.layer.10.output.dense.weight -> roberta.encoder.layer.10.output.dense.weight\n",
      "perfformer.encoder.layer.10.output.dense.bias -> roberta.encoder.layer.10.output.dense.bias\n",
      "perfformer.encoder.layer.10.output.LayerNorm.weight -> roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.10.output.LayerNorm.bias -> roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.11.attention.self.query.weight -> roberta.encoder.layer.11.attention.self.query.weight\n",
      "perfformer.encoder.layer.11.attention.self.query.bias -> roberta.encoder.layer.11.attention.self.query.bias\n",
      "perfformer.encoder.layer.11.attention.self.key.weight -> roberta.encoder.layer.11.attention.self.key.weight\n",
      "perfformer.encoder.layer.11.attention.self.key.bias -> roberta.encoder.layer.11.attention.self.key.bias\n",
      "perfformer.encoder.layer.11.attention.self.value.weight -> roberta.encoder.layer.11.attention.self.value.weight\n",
      "perfformer.encoder.layer.11.attention.self.value.bias -> roberta.encoder.layer.11.attention.self.value.bias\n",
      "perfformer.encoder.layer.11.attention.output.dense.weight -> roberta.encoder.layer.11.attention.output.dense.weight\n",
      "perfformer.encoder.layer.11.attention.output.dense.bias -> roberta.encoder.layer.11.attention.output.dense.bias\n",
      "perfformer.encoder.layer.11.attention.output.LayerNorm.weight -> roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.11.attention.output.LayerNorm.bias -> roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "perfformer.encoder.layer.11.intermediate.dense.weight -> roberta.encoder.layer.11.intermediate.dense.weight\n",
      "perfformer.encoder.layer.11.intermediate.dense.bias -> roberta.encoder.layer.11.intermediate.dense.bias\n",
      "perfformer.encoder.layer.11.output.dense.weight -> roberta.encoder.layer.11.output.dense.weight\n",
      "perfformer.encoder.layer.11.output.dense.bias -> roberta.encoder.layer.11.output.dense.bias\n",
      "perfformer.encoder.layer.11.output.LayerNorm.weight -> roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "perfformer.encoder.layer.11.output.LayerNorm.bias -> roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "lm_head.bias -> lm_head.bias\n",
      "lm_head.dense.weight -> lm_head.dense.weight\n",
      "lm_head.dense.bias -> lm_head.dense.bias\n",
      "lm_head.layer_norm.weight -> lm_head.layer_norm.weight\n",
      "lm_head.layer_norm.bias -> lm_head.layer_norm.bias\n",
      "lm_head.decoder.weight -> lm_head.decoder.weight\n",
      "lm_head.decoder.bias -> None\n"
     ]
    }
   ],
   "source": [
    "perfformer_state_dict = perfformerModel.state_dict()\n",
    "perfformer_state_dict_keys = [i for i in perfformer_state_dict.keys()]\n",
    "\n",
    "for key in perfformer_state_dict_keys:\n",
    "    robertaKeyName = key.replace(\"perfformer\", \"roberta\")\n",
    "    if robertaKeyName in roberta_state_dict_keys:\n",
    "        print(f\"{key} -> {robertaKeyName}\")\n",
    "    else:\n",
    "        print(f\"{key} -> None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do load the weights into my new network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n"
     ]
    }
   ],
   "source": [
    "# https://gist.github.com/the-bass/0bf8aaa302f9ba0d26798b11e4dd73e3\n",
    "from collections import OrderedDict\n",
    "\n",
    "converted_state_dict = OrderedDict()\n",
    "old_dict = deepcopy(roberta_state_dict)\n",
    "\n",
    "for k, v in old_dict.items():\n",
    "    newK = k.replace(\"roberta\", \"perfformer\")\n",
    "    converted_state_dict[newK] = v\n",
    "\n",
    "print(perfformerModel.load_state_dict(converted_state_dict, strict=False))\n",
    "perfformerModel.tie_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The masked word is predicted as:  IBM\n"
     ]
    }
   ],
   "source": [
    "perfformerModel.eval()\n",
    "\n",
    "masked_sentence = \"RoBERTa is a model developed by <mask>.\"\n",
    "input = robertaTokenizer.encode_plus(masked_sentence, return_tensors='pt')\n",
    "output = perfformerModel(**input)\n",
    "\n",
    "predicted_index = torch.argmax(output.logits[0, input['input_ids'][0].tolist().index(robertaTokenizer.mask_token_id)]).item()\n",
    "prediction = robertaTokenizer.decode([predicted_index])\n",
    "\n",
    "print(f\"The masked word is predicted as: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The masked word is predicted as:  IBM\n"
     ]
    }
   ],
   "source": [
    "# Make sure to put your model in evaluation mode\n",
    "robertaModel.eval()\n",
    "\n",
    "masked_sentence = \"RoBERTa is a model developed by <mask>.\"\n",
    "input = robertaTokenizer.encode_plus(masked_sentence, return_tensors='pt')\n",
    "output = robertaModel(**input)\n",
    "\n",
    "predicted_index = torch.argmax(output.logits[0, input['input_ids'][0].tolist().index(robertaTokenizer.mask_token_id)]).item()\n",
    "prediction = robertaTokenizer.decode([predicted_index])\n",
    "\n",
    "print(f\"The masked word is predicted as: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the unmasker seems to be way more easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14840319752693176,\n",
       "  'token': 11510,\n",
       "  'token_str': ' IBM',\n",
       "  'sequence': 'RoBERTa is a model developed by IBM.'},\n",
       " {'score': 0.08577851206064224,\n",
       "  'token': 1204,\n",
       "  'token_str': ' Google',\n",
       "  'sequence': 'RoBERTa is a model developed by Google.'},\n",
       " {'score': 0.07717184722423553,\n",
       "  'token': 3709,\n",
       "  'token_str': ' Microsoft',\n",
       "  'sequence': 'RoBERTa is a model developed by Microsoft.'},\n",
       " {'score': 0.05991532653570175,\n",
       "  'token': 6109,\n",
       "  'token_str': ' NASA',\n",
       "  'sequence': 'RoBERTa is a model developed by NASA.'},\n",
       " {'score': 0.03881894797086716,\n",
       "  'token': 20124,\n",
       "  'token_str': ' MIT',\n",
       "  'sequence': 'RoBERTa is a model developed by MIT.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robertaUnmasker = transformers.pipeline('fill-mask', model=robertaModel, tokenizer=robertaTokenizer)\n",
    "robertaUnmasker(\"RoBERTa is a model developed by <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14840319752693176,\n",
       "  'token': 11510,\n",
       "  'token_str': ' IBM',\n",
       "  'sequence': 'RoBERTa is a model developed by IBM.'},\n",
       " {'score': 0.08577851206064224,\n",
       "  'token': 1204,\n",
       "  'token_str': ' Google',\n",
       "  'sequence': 'RoBERTa is a model developed by Google.'},\n",
       " {'score': 0.07717184722423553,\n",
       "  'token': 3709,\n",
       "  'token_str': ' Microsoft',\n",
       "  'sequence': 'RoBERTa is a model developed by Microsoft.'},\n",
       " {'score': 0.05991532653570175,\n",
       "  'token': 6109,\n",
       "  'token_str': ' NASA',\n",
       "  'sequence': 'RoBERTa is a model developed by NASA.'},\n",
       " {'score': 0.03881894797086716,\n",
       "  'token': 20124,\n",
       "  'token_str': ' MIT',\n",
       "  'sequence': 'RoBERTa is a model developed by MIT.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfformerUnmasker = transformers.pipeline('fill-mask', model=perfformerModel, tokenizer=robertaTokenizer)\n",
    "perfformerUnmasker(\"RoBERTa is a model developed by <mask>.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check: diff the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff in weights for layer perfformer.embeddings.word_embeddings.weight: 0.0\n",
      "Diff in weights for layer perfformer.embeddings.position_embeddings.weight: 0.0\n",
      "Diff in weights for layer perfformer.embeddings.token_type_embeddings.weight: 0.0\n",
      "Diff in weights for layer perfformer.embeddings.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.embeddings.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.0.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.1.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.2.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.3.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.4.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.5.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.6.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.7.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.8.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.9.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.10.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.query.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.query.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.key.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.key.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.value.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.self.value.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.attention.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.intermediate.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.intermediate.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.output.dense.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.output.dense.bias: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.output.LayerNorm.weight: 0.0\n",
      "Diff in weights for layer perfformer.encoder.layer.11.output.LayerNorm.bias: 0.0\n",
      "Diff in weights for layer lm_head.bias: 0.0\n",
      "Diff in weights for layer lm_head.dense.weight: 0.0\n",
      "Diff in weights for layer lm_head.dense.bias: 0.0\n",
      "Diff in weights for layer lm_head.layer_norm.weight: 0.0\n",
      "Diff in weights for layer lm_head.layer_norm.bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "for (k1, v1), (k2, v2) in zip(perfformerModel.named_parameters(), robertaModel.named_parameters()):\n",
    "    if k1 == k2 or k1.replace(\"perfformer\", \"roberta\") == k2:\n",
    "        print('Diff in weights for layer {}: {}'.format(k1, torch.sum(torch.abs(v1.data - v2.data))))\n",
    "    else:\n",
    "        print(f'Key mismatch: {k1} != {k2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also check out the hidden states!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1[logits] == out2[logits]: 0.0\n",
      "out1[hidden_states][0] == out2[hidden_states][0]: 0.0\n",
      "out1[hidden_states][1] == out2[hidden_states][1]: 0.0\n",
      "out1[hidden_states][2] == out2[hidden_states][2]: 0.0\n",
      "out1[hidden_states][3] == out2[hidden_states][3]: 0.0\n",
      "out1[hidden_states][4] == out2[hidden_states][4]: 0.0\n",
      "out1[hidden_states][5] == out2[hidden_states][5]: 0.0\n",
      "out1[hidden_states][6] == out2[hidden_states][6]: 0.0\n",
      "out1[hidden_states][7] == out2[hidden_states][7]: 0.0\n",
      "out1[hidden_states][8] == out2[hidden_states][8]: 0.0\n",
      "out1[hidden_states][9] == out2[hidden_states][9]: 0.0\n",
      "out1[hidden_states][10] == out2[hidden_states][10]: 0.0\n",
      "out1[hidden_states][11] == out2[hidden_states][11]: 0.0\n",
      "out1[hidden_states][12] == out2[hidden_states][12]: 0.0\n"
     ]
    }
   ],
   "source": [
    "# inputs = robertaTokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs = {\n",
    "    \"input_ids\": torch.as_tensor([[10,10,12,5550,2233]]),\n",
    "    \"position_ids\": torch.as_tensor([[0, 1, 2, 3, 4]]),\n",
    "    # \"output_attentions\": True,\n",
    "    \"output_hidden_states\": True\n",
    "}\n",
    "\n",
    "outputRoberta = robertaModel(**inputs)\n",
    "# robertaLogits = outputRoberta.logits\n",
    "# print(outputRoberta)\n",
    "\n",
    "outputPerfformer = perfformerModel(**inputs)\n",
    "# print(outputPerfformer)\n",
    "\n",
    "def outputDiff(out1, out2):\n",
    "    for k in out1.keys():\n",
    "        if isinstance(out1[k], tuple) or isinstance(out1[k], list):\n",
    "            for idx in range(0, len(out1[k])):\n",
    "                print(\n",
    "                    f\"out1[{k}][{idx}] == out2[{k}][{idx}]: \"\n",
    "                    f\"{torch.sum(torch.abs(out1[k][idx] - out2[k][idx]))}\"\n",
    "                )\n",
    "        else:\n",
    "            print(\n",
    "                f\"out1[{k}] == out2[{k}]: \"\n",
    "                f\"{torch.sum(torch.abs(out1[k] - out2[k]))}\"\n",
    "            )\n",
    "\n",
    "outputDiff(outputRoberta, outputPerfformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the finals..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So seems the perfformer is now working as expected! Hoo-ray!\n",
    "\n",
    "Let's test the other attention implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refModel: _IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candModels[xformers-memeff]: _IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n",
      "candModels[xformers-memeff-nomask]: _IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n",
      "candModels[torch-flash-nomask]: _IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n",
      "candModels[torch-memeff-nomask]: _IncompatibleKeys(missing_keys=['lm_head.decoder.bias'], unexpected_keys=['perfformer.pooler.dense.weight', 'perfformer.pooler.dense.bias'])\n"
     ]
    }
   ],
   "source": [
    "referenceAttnImpl = 'vanilla'\n",
    "candidateAttnImpls = [\n",
    "    'xformers-memeff', 'xformers-memeff-nomask',\n",
    "    'torch-flash-nomask', 'torch-memeff-nomask'\n",
    "]\n",
    "\n",
    "# load as roberta-base\n",
    "perfformerCfgDict = {\n",
    "    \"architectures\": [\n",
    "      \"RobertaForMaskedLM\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"bos_token_id\": 0,\n",
    "    \"eos_token_id\": 2,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 768,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 3072,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"max_position_embeddings\": 514,\n",
    "    \"model_type\": \"roberta\",\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"pad_token_id\": 1,\n",
    "    \"type_vocab_size\": 1,\n",
    "    \"vocab_size\": 50265,\n",
    "    # == The following option differs! ==\n",
    "    \"attention_type\": referenceAttnImpl,\n",
    "    # BERT-like\n",
    "    \"position_embedding_type\": \"absolute-learnable\"\n",
    "}\n",
    "\n",
    "refModel = PerfformerForMaskedLM(config=PerfformerConfig(**perfformerCfgDict))\n",
    "print(f\"refModel: {refModel.load_state_dict(converted_state_dict, strict=False)}\")\n",
    "refModel.tie_weights()\n",
    "\n",
    "candModels = {}\n",
    "for attnName in candidateAttnImpls:\n",
    "    perfformerCfgDict[\"attention_type\"] = attnName\n",
    "    candModels[attnName] = PerfformerForMaskedLM(config=PerfformerConfig(**perfformerCfgDict))\n",
    "\n",
    "    print(f\"candModels[{attnName}]: {candModels[attnName].load_state_dict(converted_state_dict, strict=False)}\")\n",
    "    candModels[attnName].tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[    0, 27110, 11126, 38495,    16,    10,  1421,  2226,    30, 50264,\n",
      "             4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, {'input_ids': tensor([[    0,  1711,    61,    52,   486,    10, 50264,     6,    30,   143,\n",
      "            97,   766,     6,    74, 11362,  4045,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]\n",
      "[{'input_ids': tensor([    0, 27110, 11126, 38495,    16,    10,  1421,  2226,    30, 50264,\n",
      "            4,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}, {'input_ids': tensor([    0,  1711,    61,    52,   486,    10, 50264,     6,    30,   143,\n",
      "           97,   766,     6,    74, 11362,  4045,     4,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}]\n",
      "[{'input_ids': torch.Size([12]), 'attention_mask': torch.Size([12])}, {'input_ids': torch.Size([18]), 'attention_mask': torch.Size([18])}]\n",
      "{'input_ids': tensor([[    0, 27110, 11126, 38495,    16,    10,  1421,  2226,    30, 50264,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1],\n",
      "        [    0,  1711,    61,    52,   486,    10, 50264,     6,    30,   143,\n",
      "            97,   766,     6,    74, 11362,  4045,     4,     2,     1,     1,\n",
      "             1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': torch.Size([2, 24]), 'attention_mask': torch.Size([2, 24])}\n"
     ]
    }
   ],
   "source": [
    "testSentences = [\n",
    "    \"RoBERTa is a model developed by <mask>.\",\n",
    "    \"That which we call a <mask>, by any other name, would smell sweet.\"\n",
    "]\n",
    "\n",
    "inputs = [robertaTokenizer.encode_plus(testSentence, return_tensors='pt') for testSentence in testSentences]\n",
    "print(inputs)\n",
    "\n",
    "# squeeze or it'll complain, bad!\n",
    "for inputElem in inputs:\n",
    "    for k, v in inputElem.items():\n",
    "        inputElem[k] = v.squeeze()\n",
    "\n",
    "print(inputs)\n",
    "print([{k: vv.shape for k, vv in v.items()} for v in inputs])\n",
    "\n",
    "inputBatch = robertaTokenizer.pad(\n",
    "    inputs,\n",
    "    padding='longest',\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(inputBatch)\n",
    "print({k: v.shape for k, v in inputBatch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing xformers-memeff...\n",
      "out1[logits] == out2[logits]: 12.82652473449707\n",
      "Testing xformers-memeff-nomask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:29: UserWarning: The attention mask is not None\n",
      "  warnings.warn(message, category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out1[logits] == out2[logits]: 3653982.25\n",
      "Testing torch-flash-nomask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:253: UserWarning: Memory efficient kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:367.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:253: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:437.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:253: UserWarning: Flash attention kernel not used because: (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:369.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:253: UserWarning: Expected query, key and value to all be of dtype: {Half}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:102.)\n",
      "  context_layer = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No available kernel.  Aborting execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\ModelInspection.ipynb  38\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/NGPP/vkPredict/notebooks/ModelInspection.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m candModels[attnName]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/NGPP/vkPredict/notebooks/ModelInspection.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m candModels[attnName]\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projects/NGPP/vkPredict/notebooks/ModelInspection.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m candOutputGpu \u001b[39m=\u001b[39m candModels[attnName](\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputBatchGpu)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/NGPP/vkPredict/notebooks/ModelInspection.ipynb#X52sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m candOutput \u001b[39m=\u001b[39m moveTo(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m, candOutputGpu)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/NGPP/vkPredict/notebooks/ModelInspection.ipynb#X52sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputDiff(refOutput, candOutput)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:599\u001b[0m, in \u001b[0;36mPerfformerForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, labels, output_hidden_states)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    582\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    583\u001b[0m     input_ids: Optional[torch\u001b[39m.\u001b[39mLongTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     output_hidden_states: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor], MaskedLMOutput]:\n\u001b[0;32m    590\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[39m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[39m        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m        Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mperfformer(\n\u001b[0;32m    600\u001b[0m         input_ids,\n\u001b[0;32m    601\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    602\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    603\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    604\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m     sequence_output \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m    607\u001b[0m     prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:502\u001b[0m, in \u001b[0;36mPerfformerModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, output_hidden_states)\u001b[0m\n\u001b[0;32m    495\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[0;32m    497\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    498\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    499\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m    500\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids\n\u001b[0;32m    501\u001b[0m )\n\u001b[1;32m--> 502\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    503\u001b[0m     embedding_output,\n\u001b[0;32m    504\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    505\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states\n\u001b[0;32m    506\u001b[0m )\n\u001b[0;32m    507\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m    509\u001b[0m \u001b[39mreturn\u001b[39;00m BaseModelOutputWithPoolingAndCrossAttentions(\n\u001b[0;32m    510\u001b[0m     last_hidden_state\u001b[39m=\u001b[39msequence_output,\n\u001b[0;32m    511\u001b[0m     hidden_states\u001b[39m=\u001b[39mencoder_outputs\u001b[39m.\u001b[39mhidden_states\n\u001b[0;32m    512\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:435\u001b[0m, in \u001b[0;36mPerfformerEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states)\u001b[0m\n\u001b[0;32m    429\u001b[0m         layer_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    430\u001b[0m             layer_module,\n\u001b[0;32m    431\u001b[0m             hidden_states,\n\u001b[0;32m    432\u001b[0m             attention_mask\n\u001b[0;32m    433\u001b[0m         )\n\u001b[0;32m    434\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m         layer_output \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    436\u001b[0m             hidden_states,\n\u001b[0;32m    437\u001b[0m             attention_mask\n\u001b[0;32m    438\u001b[0m         )\n\u001b[0;32m    440\u001b[0m     hidden_states \u001b[39m=\u001b[39m layer_output\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:388\u001b[0m, in \u001b[0;36mPerfformerLayer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    384\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    385\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    386\u001b[0m     attention_mask: Optional[torch\u001b[39m.\u001b[39mFloatTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    387\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 388\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    389\u001b[0m         hidden_states,\n\u001b[0;32m    390\u001b[0m         attention_mask\n\u001b[0;32m    391\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[39m# https://huggingface.co/docs/transformers/main/glossary#feed-forward-chunking\u001b[39;00m\n\u001b[0;32m    394\u001b[0m     \u001b[39m# mathematically equivalent to calling self.feed_forward_chunk directly\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    396\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward_chunk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:367\u001b[0m, in \u001b[0;36mPerfformerAttention.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    363\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    364\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m    365\u001b[0m     attention_mask: Optional[torch\u001b[39m.\u001b[39mFloatTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    366\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 367\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    368\u001b[0m         hidden_states,\n\u001b[0;32m    369\u001b[0m         attention_mask\n\u001b[0;32m    370\u001b[0m     )\n\u001b[0;32m    371\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_output, hidden_states)\n\u001b[0;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m attention_output\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Projects\\NGPP\\vkPredict\\notebooks\\..\\model\\modeling_perfformer.py:253\u001b[0m, in \u001b[0;36mPerfformerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    250\u001b[0m         warn_once(\u001b[39m\"\u001b[39m\u001b[39mThe attention mask is not None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msdp_kernel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtorch_backend_map[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_type]):\n\u001b[1;32m--> 253\u001b[0m         context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mscaled_dot_product_attention(\n\u001b[0;32m    254\u001b[0m             query_layer, key_layer, value_layer,\n\u001b[0;32m    255\u001b[0m             attn_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    256\u001b[0m             dropout_p\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_prob \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39melse\u001b[39;49;00m \u001b[39m0.0\u001b[39;49m,\n\u001b[0;32m    257\u001b[0m             is_causal\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    258\u001b[0m         )\n\u001b[0;32m    260\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxformers-memeff-nomask\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No available kernel.  Aborting execution."
     ]
    }
   ],
   "source": [
    "refModel.eval()\n",
    "refOutput = refModel(**inputBatch)\n",
    "\n",
    "# move to gpu! Could be a view or a copy\n",
    "def moveTo(dest, batch):\n",
    "    newBatch = {}\n",
    "    for k, v in batch.items():\n",
    "        newBatch[k] = v.to(dest)\n",
    "    return newBatch\n",
    "\n",
    "inputBatchGpu = moveTo('cuda', inputBatch)\n",
    "\n",
    "for attnName in candidateAttnImpls:\n",
    "    print(f\"Testing {attnName}...\")\n",
    "    candModels[attnName].to('cuda')\n",
    "    candModels[attnName].eval()\n",
    "\n",
    "    candOutputGpu = candModels[attnName](**inputBatchGpu)\n",
    "    candOutput = moveTo('cpu', candOutputGpu)\n",
    "\n",
    "    outputDiff(refOutput, candOutput)\n",
    "    # print(refOutput)\n",
    "    # print(candOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate reference output is going to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 6]\n",
      "tensor([[-3.1715, -3.6989,  4.0094,  ..., -7.1897, -7.6822, -0.5979],\n",
      "        [-3.6039, -4.2095,  2.5274,  ..., -6.9033, -5.5549, -1.6228]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "The masked word is predicted as:  IBM flower\n"
     ]
    }
   ],
   "source": [
    "mask_pos = [inputBatch['input_ids'][i].tolist().index(robertaTokenizer.mask_token_id) for i in range(0, len(testSentences))]\n",
    "print(mask_pos)\n",
    "\n",
    "mask_pos_logits = torch.vstack([refOutput.logits[i, mask_pos[i]] for i in range(0, len(testSentences))])\n",
    "print(mask_pos_logits)\n",
    "\n",
    "predicted_index = torch.argmax(mask_pos_logits, dim=1)\n",
    "prediction = robertaTokenizer.decode(predicted_index)\n",
    "\n",
    "print(f\"The masked word is predicted as: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0911843329668045,\n",
       "  'token': 14214,\n",
       "  'token_str': ' flower',\n",
       "  'sequence': 'That which we call a flower, by any other name, would smell sweet.'},\n",
       " {'score': 0.0380307212471962,\n",
       "  'token': 17284,\n",
       "  'token_str': ' potato',\n",
       "  'sequence': 'That which we call a potato, by any other name, would smell sweet.'},\n",
       " {'score': 0.03794030100107193,\n",
       "  'token': 14099,\n",
       "  'token_str': ' lemon',\n",
       "  'sequence': 'That which we call a lemon, by any other name, would smell sweet.'},\n",
       " {'score': 0.03546350821852684,\n",
       "  'token': 3984,\n",
       "  'token_str': ' wine',\n",
       "  'sequence': 'That which we call a wine, by any other name, would smell sweet.'},\n",
       " {'score': 0.03264541178941727,\n",
       "  'token': 24410,\n",
       "  'token_str': ' grape',\n",
       "  'sequence': 'That which we call a grape, by any other name, would smell sweet.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfformerUnmasker = transformers.pipeline('fill-mask', model=perfformerModel, tokenizer=robertaTokenizer)\n",
    "perfformerUnmasker(\"That which we call a <mask>, by any other name, would smell sweet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rotary Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/notebooks\n",
    "\n",
    "For test methods see \n",
    "- xformers related pull request [here](https://github.com/facebookresearch/xformers/pull/36/files#diff-892ebce2d02f8bfe046f14c60c1754c2e259325d839816c3b8e9286bfabb0b64)\n",
    "  - tests/test_rotary_embeddings.py\n",
    "\n",
    "But seems to be buggy? Another resource: \n",
    "- https://github.com/EleutherAI/gpt-neox/blob/d8028f8e9d7a3824bee47895c28ee71ae8879234/megatron/model/positional_embeddings.py#L38\n",
    "- https://github.com/EleutherAI/gpt-neox/blob/d8028f8e9d7a3824bee47895c28ee71ae8879234/megatron/model/transformer.py#L642\n",
    "\n",
    "```python\n",
    "        if exists(self.rotary_emb):\n",
    "            if exists(self.rotary_ndims):\n",
    "                # partial rotary\n",
    "                query_rot, query_pass = (\n",
    "                    query_layer[..., : self.rotary_ndims],\n",
    "                    query_layer[..., self.rotary_ndims :],\n",
    "                )\n",
    "                key_rot, key_pass = (\n",
    "                    key_layer[..., : self.rotary_ndims],\n",
    "                    key_layer[..., self.rotary_ndims :],\n",
    "                )\n",
    "            else:\n",
    "                # full rotary\n",
    "                query_rot, key_rot = query_layer, key_layer\n",
    "\n",
    "            apply_rotary_fn = (\n",
    "                apply_rotary_pos_emb_torch if self.bf16 else apply_rotary_pos_emb\n",
    "            )\n",
    "\n",
    "            seq_len = key_layer.shape[0]\n",
    "            offset = 0\n",
    "            if exists(layer_past) and layer_past.numel() > 0:\n",
    "                offset = layer_past[0].shape[0]\n",
    "                seq_len += offset\n",
    "            cos, sin = self.rotary_emb(value_layer, seq_len=seq_len)\n",
    "            query_layer, key_layer = apply_rotary_fn(\n",
    "                query_rot, key_rot, cos, sin, offset=offset\n",
    "            )\n",
    "\n",
    "            if exists(self.rotary_ndims):\n",
    "                query_layer = torch.cat((query_layer, query_pass), dim=-1)\n",
    "                key_layer = torch.cat((key_layer, key_pass), dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rotary using xformers impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2450,  0.0412,  0.6097,  0.2446,  0.2817,  0.8046,  0.2167,\n",
       "            0.3141,  0.3358,  0.4200],\n",
       "          [-0.1789,  0.7276,  0.1119,  0.9259,  0.2643,  0.8379,  0.2453,\n",
       "            0.4618,  0.3147,  0.7494],\n",
       "          [-0.3604, -0.0615,  0.3036,  0.5874,  0.7153,  0.6680,  0.3641,\n",
       "            0.2566,  0.7189,  0.6816],\n",
       "          [-0.7689,  0.6272,  0.1594,  0.4577,  0.9483, -0.0604,  0.5580,\n",
       "            0.0189,  0.8505,  0.7860],\n",
       "          [ 0.2175,  0.6821,  0.3225,  0.9290,  0.2087, -0.4007,  0.5688,\n",
       "            0.1402,  0.9825,  0.8179]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(x, cos, sin):\n",
    "    # NOTE: This could probably be moved to Triton\n",
    "\n",
    "    # Handle a possible sequence length mismatch in between q and k\n",
    "    cos = cos[:, :, : x.shape[-2], :]\n",
    "    sin = sin[:, :, : x.shape[-2], :]\n",
    "\n",
    "    return (x * cos) + (rotate_half(x) * sin)\n",
    "\n",
    "# assumes x: (bsz, num_heads, seq_len, head_size)\n",
    "def cos_sin_tables(d_model, max_seq_len):\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "\n",
    "    t = torch.arange(\n",
    "        max_seq_len, dtype=torch.float32\n",
    "    )\n",
    "    freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "    cos_cached = emb.cos()[None, None, :, :]\n",
    "    sin_cached = emb.sin()[None, None, :, :]\n",
    "\n",
    "    return (cos_cached, sin_cached)\n",
    "\n",
    "# apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached)\n",
    "\n",
    "q_test = torch.rand(1, 1, 5, 10)\n",
    "cos_cached, sin_cached = cos_sin_tables(10, 20)\n",
    "apply_rotary_pos_emb(q_test, cos_cached, sin_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rotary using gpt-neox\n",
    "\n",
    "https://github.com/EleutherAI/gpt-neox/blob/d8028f8e9d7a3824bee47895c28ee71ae8879234/megatron/model/positional_embeddings.py#L38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.2450,  0.0412,  0.6097,  0.2446,  0.2817,  0.8046,  0.2167,\n",
       "             0.3141,  0.3358,  0.4200]]],\n",
       " \n",
       " \n",
       "         [[[-0.1789,  0.7276,  0.1119,  0.9259,  0.2643,  0.8379,  0.2453,\n",
       "             0.4618,  0.3147,  0.7494]]],\n",
       " \n",
       " \n",
       "         [[[-0.3604, -0.0615,  0.3036,  0.5874,  0.7153,  0.6680,  0.3641,\n",
       "             0.2566,  0.7189,  0.6816]]],\n",
       " \n",
       " \n",
       "         [[[-0.7689,  0.6272,  0.1594,  0.4577,  0.9483, -0.0604,  0.5580,\n",
       "             0.0189,  0.8505,  0.7860]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2175,  0.6821,  0.3225,  0.9290,  0.2087, -0.4007,  0.5688,\n",
       "             0.1402,  0.9825,  0.8179]]]]),\n",
       " tensor([[[[ 0.2450,  0.0412,  0.6097,  0.2446,  0.2817,  0.8046,  0.2167,\n",
       "             0.3141,  0.3358,  0.4200]]],\n",
       " \n",
       " \n",
       "         [[[-0.1789,  0.7276,  0.1119,  0.9259,  0.2643,  0.8379,  0.2453,\n",
       "             0.4618,  0.3147,  0.7494]]],\n",
       " \n",
       " \n",
       "         [[[-0.3604, -0.0615,  0.3036,  0.5874,  0.7153,  0.6680,  0.3641,\n",
       "             0.2566,  0.7189,  0.6816]]],\n",
       " \n",
       " \n",
       "         [[[-0.7689,  0.6272,  0.1594,  0.4577,  0.9483, -0.0604,  0.5580,\n",
       "             0.0189,  0.8505,  0.7860]]],\n",
       " \n",
       " \n",
       "         [[[ 0.2175,  0.6821,  0.3225,  0.9290,  0.2087, -0.4007,  0.5688,\n",
       "             0.1402,  0.9825,  0.8179]]]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat(\n",
    "        (-x2, x1), dim=x1.ndim - 1\n",
    "    )  # dim=-1 triggers a bug in earlier torch versions\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n",
    "    cos, sin = (\n",
    "        cos[offset : q.shape[0] + offset, ...],\n",
    "        sin[offset : q.shape[0] + offset, ...],\n",
    "    )\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "class RotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_seq_len, base=10000, precision=torch.half):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "        self.precision = precision\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.dim = dim\n",
    "\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        cos_cached, sin_cached, inv_freq = self._prepare_cache(\n",
    "            max_seq_len, precision, base\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.cos_cached = cos_cached\n",
    "        self.sin_cached = sin_cached\n",
    "\n",
    "    def _prepare_cache(self, seq_len, precision, base):\n",
    "        # precompute cos_cached, sin_cached in fp32\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n",
    "\n",
    "        t = torch.arange(seq_len).type_as(inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "        cos_cached = emb.cos()[:, None, None, :]\n",
    "        sin_cached = emb.sin()[:, None, None, :]\n",
    "\n",
    "        return (\n",
    "            cos_cached.to(precision),\n",
    "            sin_cached.to(precision),\n",
    "            inv_freq.to(precision),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seq_dim=0, seq_len=None):\n",
    "        if seq_len is None:\n",
    "            seq_len = x.shape[seq_dim]\n",
    "\n",
    "        assert seq_len <= self.max_seq_len\n",
    "\n",
    "        if seq_len != self.max_seq_len:\n",
    "            # y, z, _ = self._prepare_cache(seq_len, self.precision, self.base)\n",
    "            return (\n",
    "                self.cos_cached[:seq_len, ...].to(x.device),\n",
    "                self.sin_cached[:seq_len, ...].to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            return self.cos_cached.to(x.device), self.sin_cached.to(x.device)\n",
    "\n",
    "# (bsz, num_heads, seq_len, head_size) => (seqlen, bsz, num_heads, head_size)\n",
    "q_test_correct = q_test.permute(2, 0, 1, 3)\n",
    "rotary_emb = RotaryEmbedding(10, 50, precision=torch.float32)\n",
    "cos_table, sin_table = rotary_emb(q_test_correct, seq_dim=0)\n",
    "\n",
    "apply_rotary_pos_emb(q_test_correct, q_test_correct, cos_table, sin_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
